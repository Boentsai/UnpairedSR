#### general settings
name: 2017Track2
use_tb_logger: false
model: DegSRModel
scale: 4
gpu_ids: [0]
metrics: [psnr, ssim, lpips]

#### datasets
datasets:
  train:
    name: DIV2K
    mode: UnPairedDataset
    data_type: lmdb
    color: RGB
    ratios: [200, 200]

    dataroot_tgt: /home/lzx/SRDatasets/DIV2K_train/HR/x4_half.lmdb
    dataroot_src: /home/lzx/SRDatasets/NTIRE2017/train_LR/x4_half.lmdb

    use_shuffle: true
    workers_per_gpu: 4  # per GPU
    imgs_per_gpu: 32
    tgt_size: 128
    src_size: 32
    use_flip: true
    use_rot: true

  val:
    name: 2017Track2_mini
    mode: PairedDataset
    data_type: lmdb
    color: RGB

    dataroot_src: /home/lzx/SRDatasets/NTIRE2017/valid_LR/x4_mini.lmdb
    dataroot_tgt: /home/lzx/SRDatasets/DIV2K_valid/HR/x4_mini.lmdb

#### network structures
networks:
  netDeg:
    which_network: Translator
    setting:
      nb: 16
      nf: 64
      scale: 0.25
      zero_tail: true
    pretrain: 
      path: ~
      strict_load: true

  netSR:
    which_network: EDSR
    setting:
      nf: 64
      nb: 16
      res_scale: 1
      upscale: 4
    pretrain: 
      path: ../../../checkpoints/EDSR/edsr_baseline_x4-new.pt
      strict_load: true
  
#### training settings: learning rate scheme, loss
train:
  resume_state: ~
  D_ratio: 1
  max_grad_norm: 50
  buffer_size: 0

  optim_deg: false
  optim_sr: true

  niter: 200000
  warmup_iter: -1  # no warm up

  manual_seed: 0
  val_freq: !!float 5e3

  losses:
    sr_pix_sr: 
      type: L1Loss
      weight: 1.0

  optimizers:
    netSR:
      type: Adam
      lr: !!float 2e-4

  schedulers:
    default:
      type: MultiStepRestartLR
      milestones: [50000, 100000, 150000]
      gamma: 0.5

#### logger
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
  
